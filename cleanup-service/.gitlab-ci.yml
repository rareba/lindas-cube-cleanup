# LINDAS Cube Cleanup Service - GitLab CI/CD Configuration
#
# This pipeline provides:
# - Scheduled cleanup jobs (weekly by default)
# - Manual cleanup and restore triggers
# - Environment-specific configurations
#
# Required CI/CD Variables (set in GitLab project settings):
# - SPARQL_QUERY_ENDPOINT: SPARQL query endpoint URL
# - SPARQL_UPDATE_ENDPOINT: SPARQL update endpoint URL
# - SPARQL_USERNAME: (optional) Username for basic auth
# - SPARQL_PASSWORD: (optional) Password for basic auth
# - TRIPLESTORE_TYPE: fuseki, stardog, or graphdb
# - CLEANUP_GRAPHS: Comma-separated list of graph URIs
# - AWS_ACCESS_KEY_ID: (optional) For S3 backups
# - AWS_SECRET_ACCESS_KEY: (optional) For S3 backups
# - AWS_S3_BUCKET: (optional) S3 bucket name

stages:
  - test
  - preview
  - cleanup
  - restore

variables:
  NODE_VERSION: "18"
  VERSIONS_TO_KEEP: "2"
  BACKUP_RETENTION_DAYS: "90"

# Default settings
default:
  image: node:${NODE_VERSION}
  before_script:
    - cd cleanup-service
    - npm ci --silent
  cache:
    key: ${CI_COMMIT_REF_SLUG}
    paths:
      - cleanup-service/node_modules/

# Test triplestore connection
test-connection:
  stage: test
  script:
    - node src/cli.js test-connection
  rules:
    - if: $CI_PIPELINE_SOURCE == "merge_request_event"
    - if: $CI_PIPELINE_SOURCE == "push"
    - if: $CI_PIPELINE_SOURCE == "web"

# Preview what would be deleted (dry-run)
preview-cleanup:
  stage: preview
  script:
    - |
      node src/cli.js cleanup \
        --dry-run \
        --graph ${CLEANUP_GRAPHS} \
        --keep ${VERSIONS_TO_KEEP}
  artifacts:
    reports:
      dotenv: cleanup-service/preview.env
  rules:
    - if: $CI_PIPELINE_SOURCE == "schedule"
    - if: $CI_PIPELINE_SOURCE == "web"
      when: manual

# Run cleanup (scheduled - weekly on Sunday at 2 AM)
scheduled-cleanup:
  stage: cleanup
  script:
    - |
      echo "Starting scheduled cleanup..."
      echo "Graphs: ${CLEANUP_GRAPHS}"
      echo "Versions to keep: ${VERSIONS_TO_KEEP}"

      node src/cli.js cleanup \
        --graph ${CLEANUP_GRAPHS} \
        --keep ${VERSIONS_TO_KEEP}
  rules:
    - if: $CI_PIPELINE_SOURCE == "schedule"
  environment:
    name: production
  artifacts:
    paths:
      - cleanup-service/logs/
    expire_in: 30 days

# Manual cleanup trigger
manual-cleanup:
  stage: cleanup
  script:
    - |
      echo "Starting manual cleanup..."
      echo "Graphs: ${CLEANUP_GRAPHS}"
      echo "Versions to keep: ${VERSIONS_TO_KEEP}"
      echo "Dry run: ${DRY_RUN:-false}"

      if [ "${DRY_RUN}" = "true" ]; then
        node src/cli.js cleanup --dry-run --graph ${CLEANUP_GRAPHS} --keep ${VERSIONS_TO_KEEP}
      else
        node src/cli.js cleanup --graph ${CLEANUP_GRAPHS} --keep ${VERSIONS_TO_KEEP}
      fi
  rules:
    - if: $CI_PIPELINE_SOURCE == "web"
      when: manual
  environment:
    name: production
  artifacts:
    paths:
      - cleanup-service/logs/
    expire_in: 30 days

# List available backups
list-backups:
  stage: preview
  script:
    - node src/cli.js list-backups --json > backups.json
    - cat backups.json
  artifacts:
    paths:
      - cleanup-service/backups.json
    expire_in: 7 days
  rules:
    - if: $CI_PIPELINE_SOURCE == "web"
      when: manual

# Restore from backup (manual trigger)
restore-backup:
  stage: restore
  script:
    - |
      if [ -z "${BACKUP_PATH}" ] || [ -z "${TARGET_GRAPH}" ]; then
        echo "ERROR: BACKUP_PATH and TARGET_GRAPH variables must be set"
        exit 1
      fi

      echo "Restoring backup..."
      echo "Backup: ${BACKUP_PATH}"
      echo "Target graph: ${TARGET_GRAPH}"
      echo "Overwrite: ${OVERWRITE:-false}"

      OVERWRITE_FLAG=""
      if [ "${OVERWRITE}" = "true" ]; then
        OVERWRITE_FLAG="--overwrite"
      fi

      node src/cli.js restore "${BACKUP_PATH}" \
        --graph "${TARGET_GRAPH}" \
        ${OVERWRITE_FLAG}
  rules:
    - if: $CI_PIPELINE_SOURCE == "web"
      when: manual
  environment:
    name: production

# Cleanup old backups
cleanup-old-backups:
  stage: cleanup
  script:
    - |
      echo "Cleaning up backups older than ${BACKUP_RETENTION_DAYS} days..."
      node -e "
        const { createStorage } = require('./src/backup');
        const { createLogger } = require('./src/utils/logger');

        createLogger({ level: 'info', console: true });

        const storage = createStorage({
          type: process.env.BACKUP_TYPE || 'local',
          retentionDays: parseInt(process.env.BACKUP_RETENTION_DAYS || '90', 10),
          local: { path: './backups' },
          s3: {
            bucket: process.env.AWS_S3_BUCKET,
            region: process.env.AWS_REGION || 'eu-central-1'
          }
        });

        storage.initialize().then(() => storage.cleanupOldBackups())
          .then(count => console.log('Deleted', count, 'old backups'))
          .catch(err => { console.error(err); process.exit(1); });
      "
  rules:
    - if: $CI_PIPELINE_SOURCE == "schedule"

# ====================
# Environment-specific jobs
# ====================

# Staging environment cleanup
staging-cleanup:
  stage: cleanup
  variables:
    SPARQL_QUERY_ENDPOINT: ${STAGING_SPARQL_QUERY_ENDPOINT}
    SPARQL_UPDATE_ENDPOINT: ${STAGING_SPARQL_UPDATE_ENDPOINT}
    CLEANUP_GRAPHS: ${STAGING_CLEANUP_GRAPHS}
  script:
    - node src/cli.js cleanup --graph ${CLEANUP_GRAPHS} --keep ${VERSIONS_TO_KEEP}
  rules:
    - if: $CI_PIPELINE_SOURCE == "web" && $ENVIRONMENT == "staging"
      when: manual
  environment:
    name: staging

# Production environment cleanup (requires approval)
production-cleanup:
  stage: cleanup
  variables:
    SPARQL_QUERY_ENDPOINT: ${PROD_SPARQL_QUERY_ENDPOINT}
    SPARQL_UPDATE_ENDPOINT: ${PROD_SPARQL_UPDATE_ENDPOINT}
    CLEANUP_GRAPHS: ${PROD_CLEANUP_GRAPHS}
  script:
    - node src/cli.js cleanup --graph ${CLEANUP_GRAPHS} --keep ${VERSIONS_TO_KEEP}
  rules:
    - if: $CI_PIPELINE_SOURCE == "web" && $ENVIRONMENT == "production"
      when: manual
  environment:
    name: production
  # Uncomment to require manual approval:
  # needs:
  #   - job: preview-cleanup
  #     artifacts: true
